{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b6f37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import OllamaLLM\n",
    "llm = OllamaLLM(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "069e61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "def convert_pdf_to_markdown(file_path: str) -> str:\n",
    "    input_doc_path = Path(file_path)\n",
    "    \n",
    "    if not input_doc_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_doc_path}\")\n",
    "\n",
    "    # Prepare pipeline options\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.do_ocr = True\n",
    "    pipeline_options.do_table_structure = True\n",
    "    pipeline_options.table_structure_options.do_cell_matching = True\n",
    "\n",
    "    # Initialize document converter\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(\n",
    "                pipeline_options=pipeline_options\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    conv_result = doc_converter.convert(input_doc_path)\n",
    "    \n",
    "    # Return markdown as text\n",
    "    markdown_output = conv_result.document.export_to_markdown()\n",
    "    cleaned_markdown = re.sub(r'<!--\\s*image\\s*-->', '', markdown_output, flags=re.IGNORECASE)\n",
    "\n",
    "    # if not os.path.exists('output'):\n",
    "    #     os.makedirs('output')\n",
    "\n",
    "    file_path = os.path.join('output', 'sample.md')\n",
    "\n",
    "    with open('output/sample.md', 'w') as file:\n",
    "        file.write(cleaned_markdown)\n",
    "\n",
    "    with open('output/sample.md', 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def read_file(file,chunking_method=None):\n",
    "    ext = os.path.splitext(file)[-1].lower()\n",
    "    if ext == \".pdf\" and chunking_method == \"MarkdownHeaderSplitter\":\n",
    "        text = convert_pdf_to_markdown(file)\n",
    "        return text\n",
    "    \n",
    "    if ext == \".pdf\":\n",
    "        reader = PdfReader(file)\n",
    "        text = \"\\n\".join([page.extract_text() for page in reader.pages])\n",
    "        return text\n",
    "    \n",
    "    elif ext == \".md\":\n",
    "        with open(file.name, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please upload a PDF or Markdown (.md) file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1404c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from rapidfuzz import fuzz, process\n",
    "import re\n",
    "\n",
    "def prepare_pipeline_options() -> PdfPipelineOptions:\n",
    "    options = PdfPipelineOptions()\n",
    "    options.do_ocr = True\n",
    "    options.do_table_structure = True\n",
    "    options.table_structure_options.do_cell_matching = True\n",
    "    return options\n",
    "\n",
    "def normalize_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def split_text_into_candidates(text, window_size=300, step=150):\n",
    "    \"\"\"\n",
    "    Create overlapping chunks from flat text for efficient fuzzy matching.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for i in range(0, len(text), step):\n",
    "        window = text[i:i+window_size]\n",
    "        if len(window) > 20:\n",
    "            candidates.append((window, i))\n",
    "    # print(candidates)\n",
    "    return candidates\n",
    "\n",
    "def match_chunks_to_pdf(chunks, pdf_file_path):\n",
    "    \"\"\"\n",
    "    Matches the provided chunks to the PDF content and extracts metadata.\n",
    "    Handles multi-page, multi-section, and multi-bbox chunks.\n",
    "    \"\"\"\n",
    "    pdf_path = Path(pdf_file_path)\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f\"PDF file not found: {pdf_file_path}\")\n",
    "\n",
    "    pipeline_options = prepare_pipeline_options()\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(\n",
    "                pipeline_options=pipeline_options\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    conv_result = doc_converter.convert(pdf_path)\n",
    "\n",
    "    document = conv_result.document\n",
    "\n",
    "    flat_text = \"\"\n",
    "    char_map = []\n",
    "    element_meta = []\n",
    "\n",
    "    current_section = None\n",
    "    for idx, text_element in enumerate(document.texts):\n",
    "        if getattr(text_element, \"label\", None) == \"section_header\":\n",
    "            current_section = text_element.text\n",
    "        element_meta.append({\n",
    "            \"page_no\": text_element.prov[0].page_no,\n",
    "            \"bbox\": text_element.prov[0].bbox,\n",
    "            \"section_header\": current_section,\n",
    "            \"text_element_idx\": idx,\n",
    "        })\n",
    "        for char_idx, char in enumerate(text_element.text):\n",
    "            flat_text += char\n",
    "            char_map.append({\n",
    "                \"text_element_idx\": idx,\n",
    "                \"char_idx_in_element\": char_idx,\n",
    "            })\n",
    "        flat_text += \" \"\n",
    "        char_map.append({\n",
    "            \"text_element_idx\": idx,\n",
    "            \"char_idx_in_element\": len(text_element.text),\n",
    "        })\n",
    "\n",
    "    norm_flat_text = normalize_text(flat_text)\n",
    "\n",
    "    # Pre-split flat text into overlapping candidates for fuzzy match\n",
    "    candidates = split_text_into_candidates(norm_flat_text)\n",
    "\n",
    "    candidate_texts = [c[0] for c in candidates]\n",
    "    candidate_offsets = [c[1] for c in candidates]\n",
    "\n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        norm_chunk = normalize_text(chunk)\n",
    "        match = re.search(re.escape(norm_chunk), norm_flat_text)\n",
    "\n",
    "        match_type = \"exact\"\n",
    "        if match:\n",
    "            start, end = match.start(), match.end()\n",
    "        else:\n",
    "            match_type = \"fuzzy\"\n",
    "            fuzzy_result = process.extractOne(\n",
    "                norm_chunk,\n",
    "                candidate_texts,\n",
    "                scorer=fuzz.partial_ratio,\n",
    "                score_cutoff=85\n",
    "            )\n",
    "            if fuzzy_result:\n",
    "                best_match, score, idx = fuzzy_result\n",
    "                offset = candidate_offsets[idx]\n",
    "                local_start = norm_flat_text.find(best_match, offset, offset + len(best_match) + 20)\n",
    "                if local_start == -1:\n",
    "                    # Add empty fallback result\n",
    "                    results.append({\n",
    "                        \"original_chunk\": chunk.strip(),\n",
    "                        \"matched_text\": \"\",\n",
    "                        \"match_type\": \"none\",\n",
    "                        \"pages\": [],\n",
    "                        \"bounding_boxes\": [],\n",
    "                        \"section_headers\": []\n",
    "                    })\n",
    "                    continue\n",
    "                start, end = local_start, local_start + len(best_match)\n",
    "            else:\n",
    "                # Add empty fallback result\n",
    "                results.append({\n",
    "                    \"original_chunk\": chunk.strip(),\n",
    "                    \"matched_text\": \"\",\n",
    "                    \"match_type\": \"none\",\n",
    "                    \"pages\": [],\n",
    "                    \"bounding_boxes\": [],\n",
    "                    \"section_headers\": []\n",
    "                })\n",
    "                continue\n",
    "\n",
    "        involved_elements = set()\n",
    "        involved_pages = set()\n",
    "        involved_bboxes = []\n",
    "        involved_sections = set()\n",
    "        matched_text = flat_text[start:end]\n",
    "\n",
    "        for i in range(start, end):\n",
    "            if i >= len(char_map): continue\n",
    "            mapping = char_map[i]\n",
    "            idx = mapping[\"text_element_idx\"]\n",
    "            meta = element_meta[idx]\n",
    "            involved_elements.add(idx)\n",
    "            involved_pages.add(meta[\"page_no\"])\n",
    "            involved_bboxes.append(meta[\"bbox\"])\n",
    "            if meta[\"section_header\"]:\n",
    "                involved_sections.add(meta[\"section_header\"])\n",
    "\n",
    "        bbox_info = []\n",
    "        for idx in sorted(involved_elements):\n",
    "            meta = element_meta[idx]\n",
    "            bbox = meta[\"bbox\"]\n",
    "            bbox_info.append({\n",
    "                \"page_no\": meta[\"page_no\"],\n",
    "                \"bbox\": {\n",
    "                    \"l\": bbox.l,\n",
    "                    \"t\": bbox.t,\n",
    "                    \"r\": bbox.r,\n",
    "                    \"b\": bbox.b\n",
    "                }\n",
    "            })\n",
    "\n",
    "        results.append({\n",
    "            \"original_chunk\": chunk.strip(),\n",
    "            \"matched_text\": matched_text.strip(),\n",
    "            \"match_type\": match_type,\n",
    "            \"pages\": sorted(list(involved_pages)),\n",
    "            \"bounding_boxes\": bbox_info,\n",
    "            \"section_headers\": list(involved_sections)\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8aa597ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='rag_collection')]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from httpx import Timeout\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"http://localhost:6333\",  # or your Qdrant server URL\n",
    "    timeout= 9999.0  # Increase timeout to 30 seconds\n",
    ")\n",
    "\n",
    "\n",
    "#qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "sen_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "try:\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=\"rag_collection\",\n",
    "        vectors_config={\n",
    "            \"size\": 384,\n",
    "            \"distance\": \"Cosine\"\n",
    "        },\n",
    "        optimizers_config={\"default_segment_number\": 1},\n",
    "        on_disk_payload=True\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "vectorstore = Qdrant(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"rag_collection\",\n",
    "    embeddings=embeddings_model\n",
    ")\n",
    "collections = qdrant_client.get_collections()\n",
    "print(collections)\n",
    "\n",
    "# def store_embeddings(text_chunks, file_id):\n",
    "#     metadatas = [\n",
    "#         {\"file_id\": file_id, \"chunk_num\": i + 1}\n",
    "#         for i in range(len(text_chunks))\n",
    "#     ]\n",
    "#     vectorstore.add_texts(texts=text_chunks, metadatas=metadatas)\n",
    "#     print(f\"Successfully stored {len(text_chunks)} chunks in vectorstore.\")\n",
    "\n",
    "def store_embeddings(chunks, file_id, pdf_path):\n",
    "    chunk_metadata = match_chunks_to_pdf(chunks, pdf_path)\n",
    "\n",
    "    metadatas = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        base_meta = {\n",
    "            \"file_id\": file_id,\n",
    "            \"chunk_num\": i + 1,\n",
    "            \"original_chunk\": chunk.strip()\n",
    "        }\n",
    "\n",
    "        # Find the matching result by original chunk\n",
    "        matched_meta = next(\n",
    "            (item for item in chunk_metadata if normalize_text(item[\"original_chunk\"]) == normalize_text(chunk)),\n",
    "            None\n",
    "        )\n",
    "\n",
    "        if matched_meta:\n",
    "            base_meta.update({\n",
    "                \"matched_text\": matched_meta.get(\"matched_text\", \"\"),\n",
    "                \"match_type\": matched_meta.get(\"match_type\", \"none\"),\n",
    "                \"pages\": matched_meta.get(\"pages\", []),\n",
    "                \"bounding_boxes\": matched_meta.get(\"bounding_boxes\", []),\n",
    "                \"section_headers\": matched_meta.get(\"section_headers\", [])\n",
    "            })\n",
    "        else:\n",
    "            # In case no metadata was returned for a chunk at all (shouldn't happen now)\n",
    "            base_meta.update({\n",
    "                \"matched_text\": \"\",\n",
    "                \"match_type\": \"none\",\n",
    "                \"pages\": [],\n",
    "                \"bounding_boxes\": [],\n",
    "                \"section_headers\": []\n",
    "            })\n",
    "\n",
    "        metadatas.append(base_meta)\n",
    "\n",
    "    vectorstore.add_texts(texts=chunks, metadatas=metadatas)\n",
    "    print(f\"Successfully stored {len(chunks)} chunks in vectorstore with metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17300584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/diva001/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def semantic_chunking(text, sen_model, threshold=0.75, window_before=1, window_after=1, max_chunk_len=300):\n",
    "    import numpy as np\n",
    "    import nltk\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentence_groups = []\n",
    "    for i in range(len(sentences)):\n",
    "        context = sentences[max(0, i - window_before): min(len(sentences), i + window_after + 1)]\n",
    "        sentence_groups.append(\" \".join(context))\n",
    "\n",
    "    embeddings = sen_model.encode(sentence_groups, normalize_embeddings=True)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    current_len = len(sentences[0])\n",
    "\n",
    "    for i in range(1, len(sentences)):\n",
    "        sim = cosine_similarity([embeddings[i - 1]], [embeddings[i]])[0][0]\n",
    "        sentence_len = len(sentences[i])\n",
    "\n",
    "        if sim >= threshold and (current_len + sentence_len) <= max_chunk_len:\n",
    "            current_chunk.append(sentences[i])\n",
    "            current_len += sentence_len\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentences[i]]\n",
    "            current_len = sentence_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def sentence_similarity_chunking(text, sen_model, threshold=0.3, max_chunk_len=300):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    embeddings = sen_model.encode(sentences, normalize_embeddings=True)\n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    current_len = len(sentences[0])\n",
    "\n",
    "    for i in range(1, len(sentences)):\n",
    "        sim = cosine_similarity([embeddings[i - 1]], [embeddings[i]])[0][0]\n",
    "        next_len = current_len + len(sentences[i])\n",
    "\n",
    "        if sim >= threshold and next_len <= max_chunk_len:\n",
    "            current_chunk.append(sentences[i])\n",
    "            current_len = next_len\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentences[i]]\n",
    "            current_len = len(sentences[i])\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def agentic_chunking(text):\n",
    "    from phi.agent import Agent\n",
    "    from phi.document.chunking.agentic import AgenticChunking\n",
    "    from phi.model.ollama import Ollama\n",
    "    agent = Agent(\n",
    "    model=Ollama(id=\"llama3.2\"),\n",
    "    search_knowledge=True\n",
    ")\n",
    " \n",
    "# Step 3: Create AgenticChunking instance\n",
    "    chunker = AgenticChunking(model=agent.model)\n",
    " \n",
    "# Step 4: Perform chunking on extracted text\n",
    "    chunks = chunker.chunk(text)  # Returns a list of DocumentChunks\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82d094aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_text(text, chunking_method=\"RecursiveTextSplitter\",model=None):\n",
    "    if chunking_method == \"MarkdownHeaderSplitter\":\n",
    "        splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n",
    "            (\"#\" , \"Header 1\"),\n",
    "            (\"##\" , \"Header 2\"),\n",
    "            (\"###\" , \"Header 3\"),\n",
    "            (\"####\" , \"Header 4\"),\n",
    "            (\"#####\" , \"Header 5\"),\n",
    "            (\"######\" , \"Header 6\"),\n",
    "        ])\n",
    "        docs = splitter.split_text(text)\n",
    "        recursive_splitter = RecursiveCharacterTextSplitter(chunk_overlap=50)\n",
    "        chunks = []\n",
    "        for doc in docs:\n",
    "            sub_chunks = recursive_splitter.split_text(doc.page_content)\n",
    "            chunks.extend(sub_chunks)\n",
    "        return chunks\n",
    "    \n",
    "    # Case: Recursive Chunking (default/fallback)\n",
    "    elif chunking_method == \"RecursiveTextSplitter\":\n",
    "        chunker = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        return chunker.split_text(text)\n",
    "\n",
    "    # Semantic Chunking\n",
    "    elif chunking_method == \"SemanticChunking\":\n",
    "        return semantic_chunking(text,model)\n",
    "\n",
    "    # Sentence Similarity Chunking\n",
    "    elif chunking_method == \"SentenceSimilarityChunking\":\n",
    "        return sentence_similarity_chunking(text,model)\n",
    "\n",
    "    # Agentic (Paragraph-based) Chunking\n",
    "    elif chunking_method == \"AgenticChunking\":\n",
    "        return agentic_chunking(text)\n",
    "\n",
    "    # Fallback\n",
    "    else:\n",
    "        return [text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81cbc7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an intelligent assistant. Use the following context to answer the question accurately.Don't hallucinate and generate precise answers.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c7eebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query, top_k,temp,rep_panalty):\n",
    "    llm.temperature = temp\n",
    "    llm.repeat_penalty = rep_panalty   \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever= vectorstore.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\"score_threshold\": 0.5, \"k\": top_k}\n",
    "    ),\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": custom_prompt}\n",
    "    )\n",
    "    \n",
    "    response = qa_chain({\"query\": query})\n",
    "    answer = response[\"result\"]\n",
    "    sources = response[\"source_documents\"]\n",
    "\n",
    "    chunks_text = \"\\n\\n\".join([\n",
    "    f\"Chunk {doc.metadata.get('chunk_num', i + 1)} \\n \"\n",
    "    f\"(File ID: {doc.metadata.get('file_id', 'unknown')}\\n | \"\n",
    "    f\"Page(s): {doc.metadata.get('pages', [])}\\n| \"\n",
    "    f\"Section(s): {doc.metadata.get('section_headers', [])} \\n| \"\n",
    "    f\"Bounding Box(es): {doc.metadata.get('bounding_boxes', [])}):\\n\"\n",
    "    f\"{doc.page_content}\"\n",
    "    for i, doc in enumerate(sources)\n",
    "    ])\n",
    "    print(chunks_text)\n",
    "    return f\"Output:\\n{answer}\\n\\nRetrieved Chunks:\\n{chunks_text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10e19ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_collection():\n",
    "    qdrant_client.delete_collection(collection_name=\"rag_collection\")\n",
    "    return f\"Collection {\"rag_collection\"} deleted successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55e6bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "SAVE_DIR = \"uploaded_files\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# === HELPER FUNCTIONS ===\n",
    "def clear_folder(folder_path):\n",
    "    \"\"\"Deletes all files/folders in the directory.\"\"\"\n",
    "    for item in os.listdir(folder_path):\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            shutil.rmtree(item_path)\n",
    "        else:\n",
    "            os.remove(item_path)\n",
    "\n",
    "def save_pdf(file_path):\n",
    "    \"\"\"Save uploaded PDF to SAVE_DIR and return path + status.\"\"\"\n",
    "    if file_path:\n",
    "        clear_folder(SAVE_DIR)\n",
    "        file_name = os.path.basename(file_path)\n",
    "        save_path = os.path.join(SAVE_DIR, file_name)\n",
    "        shutil.move(file_path, save_path)\n",
    "        return save_path, f\"File saved at: {save_path}\"\n",
    "    return None, \"No file uploaded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e3529d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    def process_and_store(filepath, chunking_method):\n",
    "        file_id = filepath\n",
    "        status_message = f\"Processing file: {file_id}\\n\"\n",
    "        status_message += f\"Chunking method selected: {chunking_method}\\n\"\n",
    "        try:\n",
    "            qdrant_client.create_collection(\n",
    "                collection_name=\"rag_collection\",\n",
    "                vectors_config={\"size\": 384, \"distance\": \"Cosine\"},\n",
    "                optimizers_config={\"default_segment_number\": 1},\n",
    "                on_disk_payload=True\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        text = read_file(filepath,chunking_method)\n",
    "        chunks = split_text(text, chunking_method=chunking_method, model=sen_model)\n",
    "        status_message += f\"Text split into {len(chunks)} chunks using '{chunking_method}'.\\n\"\n",
    "        \n",
    "        print(\"\\n=====Chunks Preview =====\\n\")\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*80}\")\n",
    "\n",
    "        store_embeddings(chunks, file_id,filepath)\n",
    "        status_message += f\"Successfully stored {len(chunks)} chunks in vectorstore.\"\n",
    "        return status_message\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(\"Error occurred:\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as app:\n",
    "    with gr.Row():\n",
    "        file_input =  gr.File(label=\"Upload PDF\", type=\"filepath\", file_types=[\".pdf\"])\n",
    "        save_button = gr.Button(\"Save File\")\n",
    "        file_status = gr.Textbox(label=\"File Save Status\", interactive=False)\n",
    "    \n",
    "    file_path_state = gr.State()\n",
    "\n",
    "    save_button.click(\n",
    "        fn=save_pdf,\n",
    "        inputs=[file_input],\n",
    "        outputs=[file_path_state, file_status]\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        chunking_method_input = gr.Dropdown(\n",
    "            choices=[\n",
    "                \"RecursiveTextSplitter\",\n",
    "                \"MarkdownHeaderSplitter\",\n",
    "                \"SemanticChunking\",\n",
    "                \"SentenceSimilarityChunking\",\n",
    "                \"AgenticChunking\"\n",
    "            ],\n",
    "            label=\"Select Chunking Method\",\n",
    "            value=\"RecursiveTextSplitter\"\n",
    "        )\n",
    "        delete_btn = gr.Button(\"Delete Collection\")\n",
    "        popup = gr.Textbox(visible=True, label=\"Status of deletion\", interactive=False)\n",
    "        upload_btn = gr.Button(\"Process File\")\n",
    "        \n",
    "    file_output = gr.Textbox(label=\"Status\")\n",
    "\n",
    "    with gr.Row():\n",
    "        question_input = gr.Textbox(label=\"Ask a question\")\n",
    "        top_k = gr.Slider(minimum=1, maximum=10, value=5, step=1, label=\"Top-K Chunks to Retrieve\")\n",
    "        temp = gr.Slider(minimum=0.0, maximum=1.0, value=0.7, step=0.5, label=\"Temperature\")\n",
    "        rep_penalty = gr.Slider(minimum=1.0, maximum=2.0, value=1.2, step=0.1, label=\"Repetition Penalty\")\n",
    "        ask_btn = gr.Button(\"Submit\")\n",
    "\n",
    "    answer_output = gr.Textbox(label=\"Answer\")\n",
    "\n",
    "    upload_btn.click(fn=process_and_store, inputs=[file_path_state, chunking_method_input], outputs=file_output)\n",
    "    ask_btn.click(fn=ask_question, inputs=[question_input, top_k,temp,rep_penalty], outputs=answer_output)\n",
    "    delete_btn.click(fn=delete_collection, outputs=popup)\n",
    "app.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
